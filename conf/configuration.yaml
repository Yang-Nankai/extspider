scraper_parameters:
  # Number of concurrent CollectorWorker threads
  # NOTE: start with amount of cores in your CPU;
  # then readjust to improve data collection performance
  collectors_amount: 8
  # Maximum amount of extension identifiers in collection queue
  # NOTE: set this to 0 for making the queue size unlimited
  backlog_limit: 0
  # Maximum amount of time to wait for new identifiers to load
  timeout_seconds: 3600
  # Frequency of progress updates
  update_seconds: 60
  # Version of Chrome Browser
  prod_version: 122.0.6261.112
  # URL to the Chrome Web Store
  store_url: https://chrome.google.com/webstore/category/extensions
  # Chrome request category id
  chrome_category_request_id: zTyKYc
  # Chrome detail request id
  chrome_detail_request_id: xY2Ddd
  # Once request extension details number
  chrome_scraper_once_num: 128
  # Proxies
  proxies: {
     "http": "http://127.0.0.1:15777"
  }

feishu_parameters:
  webhook_url: https://open.feishu.cn/open-apis/bot/v2/hook/a3c4f648-a30a-4dc2-b724-d705c8d2be81

log_parameters:
  version: 1
  disable_existing_loggers: false
  formatters:
    simple:
      format: '%(name)s - %(levelname)s - %(message)s'
    time:
      format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  handlers:
    stderr:
      class: logging.StreamHandler
      level: ERROR
      formatter: simple
      stream: ext://sys.stdout
    file:
      class: logging.handlers.RotatingFileHandler
      level: ERROR
      formatter: time
      filename: logs/runtime.log
      maxBytes: 10000000
      backupCount: 3
      encoding: utf8
  loggers:
    root:
      level: DEBUG
      handlers:
      - stderr
      - file